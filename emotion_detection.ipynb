{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NAMYp4I4HgEgZZCWaCAijS5DOQlA-04N",
      "authorship_tag": "ABX9TyNu5/XMW4Agf9sQNZVzAAF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adapateja/Machine-learning/blob/main/emotion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJXfZF5e5C7O",
        "outputId": "a1284f2f-d243-4dad-8180-ee65472a8744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvvYHydN5jxK",
        "outputId": "95a73359-ccb2-4ec6-d595-f2aa06b92751"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUrqmxsm5r4T",
        "outputId": "f9547ac8-5ce6-45c2-ef2c-c38ebc80f7ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tkzVUUQ5ugS",
        "outputId": "71aaba18-6ecd-44a8-a88a-492c15c4ea4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mMyDrive\u001b[0m/  \u001b[01;34mShareddrives\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZpXH8965wT7",
        "outputId": "34071e50-471b-4344-b7fc-2a655a425de2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'MyDrive'\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRzBcqgv5z04",
        "outputId": "13faeb47-6cdd-4362-aed1-ea51718ffed3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mClassroom\u001b[0m/\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/\n",
            "'Copy of speech-emotion-recognition-ravdess-data.zip'\n",
            " \u001b[01;34mDataset\u001b[0m/\n",
            "'DLD assignment(21PA1A05F3).pdf'\n",
            " IMG-20221015-WA0006.jpg\n",
            "'Wave Optics - Diffraction.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzl39FZe51aK",
        "outputId": "68c3b427-2b24-4457-f57c-eaa8f5ce63b7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZacilMb854vW",
        "outputId": "8a333026-926a-4ff5-b101-a2918846f9b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mspeech-emotion-recognition-ravdess-data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd speech-emotion-recognition-ravdess-data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn7C4TkV564g",
        "outputId": "28f8b4b0-4c1e-40f6-a558-deb5a454be30"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Dataset/speech-emotion-recognition-ravdess-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcJtMR2K596H",
        "outputId": "4e383cae-c4fe-4263-f462-62ed44fa0aaa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mActor_01\u001b[0m/  \u001b[01;34mActor_05\u001b[0m/  \u001b[01;34mActor_09\u001b[0m/  \u001b[01;34mActor_13\u001b[0m/  \u001b[01;34mActor_17\u001b[0m/  \u001b[01;34mActor_21\u001b[0m/\n",
            "\u001b[01;34mActor_02\u001b[0m/  \u001b[01;34mActor_06\u001b[0m/  \u001b[01;34mActor_10\u001b[0m/  \u001b[01;34mActor_14\u001b[0m/  \u001b[01;34mActor_18\u001b[0m/  \u001b[01;34mActor_22\u001b[0m/\n",
            "\u001b[01;34mActor_03\u001b[0m/  \u001b[01;34mActor_07\u001b[0m/  \u001b[01;34mActor_11\u001b[0m/  \u001b[01;34mActor_15\u001b[0m/  \u001b[01;34mActor_19\u001b[0m/  \u001b[01;34mActor_23\u001b[0m/\n",
            "\u001b[01;34mActor_04\u001b[0m/  \u001b[01;34mActor_08\u001b[0m/  \u001b[01;34mActor_12\u001b[0m/  \u001b[01;34mActor_16\u001b[0m/  \u001b[01;34mActor_20\u001b[0m/  \u001b[01;34mActor_24\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob \n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "8ZRoJU6E6AcK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_feature(file_name):\n",
        "    X, sample_rate = librosa.load(file_name)\n",
        "    stft=np.abs(librosa.stft(X))\n",
        "    result=np.array([])\n",
        "    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
        "    result=np.hstack((result, mfccs))\n",
        "    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "    result=np.hstack((result, chroma))\n",
        "    mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
        "    result=np.hstack((result, mel))\n",
        "    return result"
      ],
      "metadata": {
        "id": "SIkT_JTc6cjG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "\n",
        "def gender(g):\n",
        "    if int(g[0:2]) % 2 == 0:\n",
        "        return 'female'\n",
        "    else:\n",
        "        return 'male'"
      ],
      "metadata": {
        "id": "D6ibGtQK6g69"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd Actor_01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu49pFab6soa",
        "outputId": "728a6cf5-7f44-47ff-9af5-4cc898a1ae8b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Dataset/speech-emotion-recognition-ravdess-data/Actor_01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(test_size=0.2):\n",
        "    x,y=[],[]\n",
        "    for file in tqdm(glob.glob(\"/content/drive/MyDrive/Dataset/speech-emotion-recognition-ravdess-data/Actor_*/*.wav\")):\n",
        "        file_name=os.path.basename(file)\n",
        "        if(file_name.split(\"-\")[2] in emotions):\n",
        "          emotion=emotions[file_name.split(\"-\")[2]] + '_' + gender(file_name.split(\"-\")[-1])\n",
        "          feature=extract_feature(file)\n",
        "          x.append(feature)\n",
        "          y.append(emotion)\n",
        "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n",
        "     \n"
      ],
      "metadata": {
        "id": "Bn-5GvNc6jnw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CauLOZZH69yn",
        "outputId": "fe161792-a196-4195-f4c8-cd9c115b130c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1440/1440 [01:44<00:00, 13.84it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((X_train.shape[0], X_test.shape[0]))\n",
        "print(f'Features extracted: {X_train.shape[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR2elRKPAzSP",
        "outputId": "efed9d4d-7fd8-4f50-fd22-eb08605dff06"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1152, 288)\n",
            "Features extracted: 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "7Aj2mziZA6-h"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "xt = 'X.train'\n",
        "joblib.dump(X_train, xt, compress=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvdJcwnrA8fW",
        "outputId": "fcc29277-87c0-4fc0-cc99-b2a603806657"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['X.train']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''mlp_params = {'activation': 'relu', \n",
        "              'solver': 'lbfgs', \n",
        "              'hidden_layer_sizes': 1194, \n",
        "              'alpha': 0.8432377345669054, \n",
        "              'batch_size': 173, \n",
        "              'learning_rate': 'constant',\n",
        "              'max_iter':1000}'''\n",
        "lgb_params = {'num_leaves': 5, \n",
        "              'max_depth': 58, \n",
        "              'n_estimators': 14734, \n",
        "              'subsample_for_bin': 491645, \n",
        "              'min_data_in_leaf': 27, \n",
        "              'reg_alpha': 1.744123586157066, \n",
        "              'colsample_bytree': 0.6495503686746514, \n",
        "              'learning_rate': 0.8581745963346554, \n",
        "              'boosting_type': 'dart'}\n",
        "mlp2_params = {'activation': 'relu', \n",
        "              'solver': 'lbfgs', \n",
        "              'hidden_layer_sizes': 1283, \n",
        "              'alpha': 0.3849485717707319, \n",
        "              'batch_size': 163, \n",
        "              'learning_rate': 'constant',\n",
        "              'max_iter':1000}"
      ],
      "metadata": {
        "id": "naHPis24BDrK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v_params = {'estimators':[('mlp2', MLPClassifier(**mlp2_params)), \n",
        "                          ('lgb', lgb.LGBMClassifier(**lgb_params))], \n",
        "            'voting':'soft'}\n",
        "models = {}\n",
        "models['v'] = VotingClassifier(**v_params)\n",
        "#models['mlp'] = MLPClassifier(**mlp_params)\n",
        "model_abrv = {'v':'Voting Classifier: MLP2, LGB'}#,'mlp':'MLP'}"
      ],
      "metadata": {
        "id": "RRshdd2aBGBq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14, model='clf', save=True):\n",
        "    df_cm = pd.DataFrame(\n",
        "        confusion_matrix, index=class_names, columns=class_names, \n",
        "    )\n",
        "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "    try:\n",
        "        heatmap = sns.heatmap(df_cm, annot=True, ax=ax, fmt=\"d\", cmap=plt.cm.Oranges)\n",
        "    except ValueError:\n",
        "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
        "        \n",
        "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
        "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    b, t = plt.ylim() \n",
        "    b += 0.5 \n",
        "    t -= 0.5 \n",
        "    plt.ylim(b, t) \n",
        "    if save == True:\n",
        "        plt.savefig('confusion_matrix.jpg')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "no9c5ZeBBKLf"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(clf, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, models=models, save=False, print_stat=True, inc_train=False, cv=False):\n",
        "    clf_model = models[clf]\n",
        "    clf_model.fit(X_train, y_train)\n",
        "    y_pred = clf_model.predict(X_test)\n",
        "    if print_stat == True:\n",
        "        clf_report = pd.DataFrame(classification_report(y_test,y_pred, output_dict=True)).T\n",
        "        clf_report.to_csv('tuned_' + model_abrv[clf] + '_classification_report.csv')\n",
        "        print(model_abrv[clf])\n",
        "        print('\\nTest Stats\\n', classification_report(y_test,y_pred))\n",
        "        print_confusion_matrix(confusion_matrix(y_test, y_pred), unique_labels(y_test, y_pred), model=clf)\n",
        "        if inc_train == True:\n",
        "            print(model_abrv[clf])\n",
        "            print('\\nTrain Stats\\n', classification_report(y_train,clf_model.predict(X_train)))\n",
        "            print_confusion_matrix(confusion_matrix(y_train, clf_model.predict(X_train)), unique_labels(y_test, y_pred), model=clf)\n",
        "    if cv == True:\n",
        "        print(model_abrv[clf] + ' CV Accuracy:',  \n",
        "              np.mean(cross_val_score(clf_model, X_train, y_train, cv=5, scoring='accuracy')))\n",
        "    if save == True:\n",
        "        return clf_model"
      ],
      "metadata": {
        "id": "ghBQIU1NBQRO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in models.keys():\n",
        "    fmodel=model(key,save=True,print_stat=False,cv=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DcBLvoKBSlE",
        "outputId": "826bd8ad-d1f9-4a22-e32b-72dd8ac8b684"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=27, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=27\n",
            "Voting Classifier: MLP2, LGB CV Accuracy: 0.04600037643515904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = '/content'\n",
        "fmodel_path = os.path.join(save_dir, model_name)\n",
        "joblib.dump(fmodel, fmodel_path, compress=9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDOXbBLRE96v",
        "outputId": "6092c243-2eaf-493f-db68-4043027d92d6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/Emotion_Voice_Detection_Model.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}